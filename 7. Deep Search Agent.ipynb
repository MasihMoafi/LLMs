{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9327343-3e11-4a88-b798-95ff4644e2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.schema import Document\n",
    "from ollama import chat\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "EMBEDDING_MODEL = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'\n",
    "LLM_MODEL = 'gemma2:9b'\n",
    "CHUNK_SIZE = 1000\n",
    "OVERLAP = 200\n",
    "CHROMA_PERSIST_DIR = r'\\home\\Masih\\chroma_db\\chroma_db'\n",
    "\n",
    "class ChromaRAGSystem:\n",
    "    def __init__(self):\n",
    "        # Init embedding model\n",
    "        self.embeddings = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "        # Vector store instance\n",
    "        self.vector_db = None\n",
    "        \n",
    "    def build_vector_store(self, documents):\n",
    "        \"\"\"Create Chroma vector store from documents\"\"\"\n",
    "        self.vector_db = Chroma.from_documents(\n",
    "            documents=documents,\n",
    "            embedding=self.embeddings,\n",
    "            persist_directory=CHROMA_PERSIST_DIR\n",
    "        )\n",
    "        self.vector_db.persist()\n",
    "        \n",
    "    def load_vector_store(self):\n",
    "        \"\"\"Load existing Chroma vector store\"\"\"\n",
    "        self.vector_db = Chroma(\n",
    "            persist_directory=CHROMA_PERSIST_DIR,\n",
    "            embedding_function=self.embeddings\n",
    "        )\n",
    "        \n",
    "    def document_query(self, query, top_k=5):\n",
    "        \"\"\"Retrieve context from documents based on query\"\"\"\n",
    "        # Perform similarity search across all documents\n",
    "        results = self.vector_db.similarity_search(query=query, k=top_k)\n",
    "        return [doc.page_content for doc in results]\n",
    "\n",
    "class AnswerGenerator:\n",
    "    def __init__(self, rag_system):\n",
    "        self.rag = rag_system\n",
    "        \n",
    "    def generate_response(self, question):\n",
    "        \"\"\"Generate context-aware answer using LLM\"\"\"\n",
    "        # Retrieve relevant context from the best matching documents\n",
    "        context_chunks = self.rag.document_query(question)\n",
    "        context = \"\\n\".join(context_chunks)\n",
    "        \n",
    "        prompt = f\"\"\"با استفاده از متن زیر به سوال پاسخ دهید:\n",
    "{context}\n",
    "\n",
    "اگر پاسخ در متن وجود ندارد عبارت 'پاسخی یافت نشد' را برگردانید\n",
    "\n",
    "سوال: {question}\n",
    "پاسخ:\"\"\"\n",
    "        \n",
    "        response = chat(model=LLM_MODEL, messages=[{'role': 'user', 'content': prompt}])\n",
    "        return response['message']['content']\n",
    "\n",
    "def scrape_url(url):\n",
    "    \"\"\"Scrape the content from a given URL\"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract the article text (adjust this as per the specific page's structure)\n",
    "    paragraphs = soup.find_all('p')\n",
    "    article_text = \"\\n\".join([para.get_text() for para in paragraphs])\n",
    "\n",
    "    return article_text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://tosinso.com/articles/40596\"\n",
    "    article_content = scrape_url(url)\n",
    "\n",
    "    # Process the scraped content and create a vector store\n",
    "    rag_system = ChromaRAGSystem()\n",
    "\n",
    "    # Chunk the article content\n",
    "    chunks = [article_content[i:i+CHUNK_SIZE] for i in range(0, len(article_content), CHUNK_SIZE - OVERLAP)]\n",
    "    documents = [Document(page_content=chunk) for chunk in chunks]\n",
    "\n",
    "    # Build vector store\n",
    "    rag_system.build_vector_store(documents)\n",
    "\n",
    "    # Init answer generator\n",
    "    answer_engine = AnswerGenerator(rag_system)\n",
    "\n",
    "    # The query to be answered\n",
    "    query = \"تفاوت زیروکلاینت و تین کلاینت با PC در چیست؟\"\n",
    "\n",
    "    # Generate and print the response\n",
    "    answer = answer_engine.generate_response(query)\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9f2290-d16f-4722-857a-7996d4722857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_internet(query):\n",
    "    \"\"\"Search the web for the given query and return a relevant snippet.\"\"\"\n",
    "    query = query.replace(\" \", \"+\")  # Format the query for URLs\n",
    "    url = f\"https://www.google.com/search?q={query}\"\n",
    "    \n",
    "    # Send a GET request to Google (NOTE: scraping Google directly can get blocked)\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        return \"Error: Unable to retrieve data from the internet.\"\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Scrape content from search result snippets (extract the first result snippet)\n",
    "    search_results = soup.find_all('div', {'class': 'BNeawe iBp4i AP7Wnd'})\n",
    "    if search_results:\n",
    "        return search_results[0].get_text()\n",
    "    \n",
    "    return \"No relevant information found on the web.\"\n",
    "\n",
    "def generate_answer(query):\n",
    "    \"\"\"Generate an answer by first checking Wikipedia and then searching the internet.\"\"\"\n",
    "    # First, check Wikipedia for Persian content\n",
    "    wikipedia_answer = search_wikipedia(query)\n",
    "    if wikipedia_answer and \"Error\" not in wikipedia_answer:\n",
    "        return wikipedia_answer\n",
    "    \n",
    "    # If not found in Wikipedia, search the web\n",
    "    internet_answer = search_internet(query)\n",
    "    return internet_answer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"شاه عباس صفوی که بود و چه کرد؟\"\n",
    "    \n",
    "    # Get the answer from Wikipedia and Internet search\n",
    "    answer = generate_answer(query)\n",
    "    \n",
    "    # Print the answer\n",
    "    print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091053f6-8c5d-4cd7-89a2-08690ed1f2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def search_duckduckgo(query):\n",
    "    \"\"\"Search DuckDuckGo for the given query and return URLs of the top results.\"\"\"\n",
    "    query = query.replace(\" \", \"+\")  # Format the query for DuckDuckGo search URLs\n",
    "    url = f\"https://duckduckgo.com/html/?q={query}\"\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=5)\n",
    "        if response.status_code != 200:\n",
    "            print(\"Failed to fetch search results.\")\n",
    "            return []\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find all result URLs\n",
    "        search_results = soup.find_all('a', {'class': 'result__a'}, href=True)\n",
    "        urls = []\n",
    "        for result in search_results:\n",
    "            href = result['href']\n",
    "            if href.startswith('http'):\n",
    "                urls.append(href)\n",
    "        return urls[:5]  # Limit to the first 5 URLs\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching DuckDuckGo search results: {e}\")\n",
    "        return []\n",
    "\n",
    "def crawl_website(url):\n",
    "    \"\"\"Crawl a website and extract text content.\"\"\"\n",
    "    try:\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        response = requests.get(url, headers=headers, timeout=5)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        \n",
    "        # Extract text from the first few paragraphs\n",
    "        text = \" \".join([p.get_text(strip=True) for p in paragraphs[:3]])  # Limit to first 3 paragraphs\n",
    "        return text\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error crawling {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_answer(query):\n",
    "    \"\"\"Generate an answer by searching DuckDuckGo and crawling the web.\"\"\"\n",
    "    # Search DuckDuckGo for relevant URLs\n",
    "    urls = search_duckduckgo(query)\n",
    "    if not urls:\n",
    "        print(\"No search results found.\")\n",
    "        return\n",
    "\n",
    "    # Crawl the first few URLs and collect the text content\n",
    "    data = {}\n",
    "    for url in urls:\n",
    "        print(f\"Crawling: {url}\")\n",
    "        text = crawl_website(url)\n",
    "        if text:\n",
    "            data[url] = text\n",
    "        time.sleep(2)  # Delay to prevent being blocked by DuckDuckGo\n",
    "\n",
    "    # Find the most fitting answer by choosing the content from the first URL (or the most relevant content)\n",
    "    if data:\n",
    "        print(\"\\nBest answer found:\\n\")\n",
    "        return list(data.values())[0]  # Pick content from the first valid URL\n",
    "    else:\n",
    "        return \"No suitable content found.\"\n",
    "\n",
    "def save_results_to_file(content, file_path):\n",
    "    \"\"\"Save the generated answer to a text file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "        print(f\"Results saved to {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results to file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = input(\"Enter your question: \")\n",
    "    answer = generate_answer(query)\n",
    "    \n",
    "    # Save the answer to the file if it exists\n",
    "    if answer:\n",
    "        file_path = r'/home/masih/rag_data/search_results.txt'  \n",
    "        save_results_to_file(answer, file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
